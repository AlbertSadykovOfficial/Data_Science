"""

	Прогнозирование вредоносных URL-адресов.

		Дано:
			1) Данные за 120 дней, каждое наблюдение - 3 200 000 показателей.
				 При этом  1 - сайт вредоносный
				 				  -1 - сайт нормальный
			2) Библиотека Scikit-learn

		Цель:
			Можно ли доверять некоторым адресам или нет.
			Научиться работать с большим объемом данных, не помещающимся в ОЗУ.
_____________________________________________________________________________
Результат выполнения программы 
(Может быть иным при повторном выполнении, т.к алгоритм может сходиться иначе)

              precision    recall  f1-score   support
							(Точность) (Полнота)(F1-метрика)(Поддержка)
          -1       0.98      0.98      0.98     14298
           1       0.96      0.96      0.96      5702

    accuracy                           0.98     20000
   macro avg       0.97      0.97      0.97     20000
weighted avg       0.98      0.98      0.98     20000
_____________________________________________________________________________
Дешифрация:
				(Точность) (Полнота)(F1-метрика)(Поддержка)
	 1       0.96      0.96      0.96      5702

	Всего 4% вредоносных сайтов не были обнаружены (судим по точности)
	Всего 4% обвинены незаслужено (Полнота)
_____________________________________________________________________________
"""

import tarfile
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import classification_report
from sklearn.datasets import load_svmlight_file
import numpy as np

# Скачать архив можно по ссылкке:
# http://www.sysnet.ucsd.edu/projects/url/#datasets
uri = r"D:\url_svmlight.tar.gz"
tar = tarfile.open(uri, "r:gz")

classes = [-1, 1] 							# Целевая переменная - 1-безопасно, -1-небезопасен
sgd = SGDClassifier(loss="log") # Классификатор статического градиентного спуска 
n_features = 3231952 						# Кол-во показателей известно из исследованных данных
split = 5												# Остановить программу на 5 файле
i = 0

# Суть в том, чтобы не распаковывать весь архив (2,05 Гб) - это забьет память
# Данные загружаются по файлово (Я смотрел, 1 файл примерно 17 МБ)
for tarinfo in tar:
		if i > split:
			break
		if tarinfo.isfile():
				f = tar.extractfile(tarinfo.name)
				X,y = load_svmlight_file(f, n_features=n_features) # Загрузить конкретный файл
				if i < split:
						sgd.partial_fit(X, y, classes=classes) # Онлайновый алгоритм -точки данных передаются пакетами (файл за файлом)
				if i == split:
					print(classification_report(sgd.predict(X), y))
		i += 1